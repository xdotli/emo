<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%26id_list%3D2501.00038v1%26start%3D0%26max_results%3D1" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=&amp;id_list=2501.00038v1&amp;start=0&amp;max_results=1</title>
  <id>http://arxiv.org/api/JrGKluCZFgymEggBa/QwHj9OIMg</id>
  <updated>2025-02-25T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2501.00038v1</id>
    <updated>2024-12-24T09:51:00Z</updated>
    <published>2024-12-24T09:51:00Z</published>
    <title>Sound-Based Recognition of Touch Gestures and Emotions for Enhanced
  Human-Robot Interaction</title>
    <summary>  Emotion recognition and touch gesture decoding are crucial for advancing
human-robot interaction (HRI), especially in social environments where
emotional cues and tactile perception play important roles. However, many
humanoid robots, such as Pepper, Nao, and Furhat, lack full-body tactile skin,
limiting their ability to engage in touch-based emotional and gesture
interactions. In addition, vision-based emotion recognition methods usually
face strict GDPR compliance challenges due to the need to collect personal
facial data. To address these limitations and avoid privacy issues, this paper
studies the potential of using the sounds produced by touching during HRI to
recognise tactile gestures and classify emotions along the arousal and valence
dimensions. Using a dataset of tactile gestures and emotional interactions from
28 participants with the humanoid robot Pepper, we design an audio-only
lightweight touch gesture and emotion recognition model with only 0.24M
parameters, 0.94MB model size, and 0.7G FLOPs. Experimental results show that
the proposed sound-based touch gesture and emotion recognition model
effectively recognises the arousal and valence states of different emotions, as
well as various tactile gestures, when the input audio length varies. The
proposed model is low-latency and achieves similar results as well-known
pretrained audio neural networks (PANNs), but with much smaller FLOPs,
parameters, and model size.
</summary>
    <author>
      <name>Yuanbo Hou</name>
    </author>
    <author>
      <name>Qiaoqiao Ren</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Dick Botteldooren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.00038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.00038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
